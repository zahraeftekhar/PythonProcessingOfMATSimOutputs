{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SnapShot preprocessing:\n",
    "In this notebook, we process the snapshots to reconstruct the travel diaries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### adding required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from scipy.stats import gaussian_kde\n",
    "import time\n",
    "import numpy as np\n",
    "import pickle\n",
    "import concurrent.futures\n",
    "from loky import ProcessPoolExecutor\n",
    "from threading import Thread\n",
    "import logging\n",
    "import multiprocessing as mp\n",
    "from multiprocess import Pool\n",
    "from joblib import Parallel, delayed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### specifying the saving location \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "savingLoc = \"Y:/ZahraEftekhar/phase4/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we have to cluster the consecative records that have the same location (i.e., the TAZ)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cluster_records(interval,address):\n",
    "    t1= time.time()\n",
    "    clusterData = {}\n",
    "    with open(\"{a}completePLUdata_{b}sec_dict.pickle\".format(a=address,b= interval),'rb') as handle:\n",
    "        snapDataSplit = pickle.load(handle)\n",
    "    IDs = list(snapDataSplit.keys())\n",
    "    for i,ID in enumerate(IDs):\n",
    "        person = snapDataSplit[ID]\n",
    "        person = person.reset_index(drop=False, inplace=False)\n",
    "        newD = pd.DataFrame()\n",
    "        for k,v in person.groupby((person.mzr_id.shift() != person.mzr_id).cumsum()):\n",
    "            newD = newD.append(v.iloc[0,:])\n",
    "        d = np.array(newD[\"TIME\"].shift(periods=-1, fill_value=v.iloc[-1,:][\"TIME\"])) - np.array(newD[\"TIME\"])\n",
    "        d = [pd.Timedelta(d[i]).total_seconds() for i in np.arange(len(d))]\n",
    "        newD[\"duration\"] = d\n",
    "        newD.columns = ['start', 'id', 'x', 'y', 'mzr_id', 'duration']\n",
    "        clusterData[ID] = newD\n",
    "    print(interval, \":  \",(time.time()-t1)//60,\"minutes\")\n",
    "    with open('{a}clusterData_{b}sec.pickle'.format(a= address,b = interval), 'wb') as handle:\n",
    "        pickle.dump(clusterData, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "637.1613354682922\n"
     ]
    }
   ],
   "source": [
    "#with this type of parallel processing execution of this cell takes only about 30 minutes for all the intervals! \n",
    "# if not using parallel processing it would be more than 2 hours!\n",
    "intervals = [60,300,600,900,1200,1500,1800,2100,2400,2700,3000,3300,3600,4500,5400,6300,7200]\n",
    "t2 = time.time()\n",
    "results = Parallel(n_jobs=6)(delayed(cluster_records)(interval,savingLoc) for interval in intervals)\n",
    "print(time.time()-t2)   "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
