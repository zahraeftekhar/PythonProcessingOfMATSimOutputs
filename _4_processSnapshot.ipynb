{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SnapShot preprocessing:\n",
    "In this notebook, we process the snapshots to reconstruct the travel diaries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### adding required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from scipy.stats import gaussian_kde\n",
    "import time\n",
    "import numpy as np\n",
    "import pickle\n",
    "# import concurrent.futures\n",
    "# from loky import ProcessPoolExecutor\n",
    "# from threading import Thread\n",
    "# import logging\n",
    "# import multiprocessing as mp\n",
    "# from multiprocess import Pool\n",
    "from joblib import Parallel, delayed\n",
    "import datetime as dt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### specifying the saving location \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "savingLoc = \"Y:/ZahraEftekhar/phase4/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clustering The successive similar records"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we have to cluster the consecative records that have the same location (i.e., the TAZ)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cluster_records(interval,address):\n",
    "    t1= time.time()\n",
    "    clusterData = {}\n",
    "    with open(\"{a}completePLUdata_{b}sec_dict.pickle\".format(a=address,b= interval),'rb') as handle:\n",
    "        snapDataSplit = pickle.load(handle)\n",
    "    IDs = list(snapDataSplit.keys())\n",
    "    for i,ID in enumerate(IDs):\n",
    "        person = snapDataSplit[ID]\n",
    "        person = person.reset_index(drop=False, inplace=False)\n",
    "        newD = pd.DataFrame()\n",
    "        for k,v in person.groupby((person.mzr_id.shift() != person.mzr_id).cumsum()):\n",
    "            newD = newD.append(v.iloc[0,:])\n",
    "        d = np.array(newD[\"TIME\"].shift(periods=-1, fill_value=v.iloc[-1,:][\"TIME\"])) - np.array(newD[\"TIME\"])\n",
    "        d = [pd.Timedelta(d[i]).total_seconds() for i in np.arange(len(d))]\n",
    "        newD[\"duration\"] = d\n",
    "        newD.columns = ['start', 'id', 'x', 'y', 'mzr_id', 'duration']\n",
    "        clusterData[ID] = newD\n",
    "    print(interval, \":  \",(time.time()-t1)//60,\"minutes\")\n",
    "    with open('{a}clusterData_{b}sec.pickle'.format(a= address,b = interval), 'wb') as handle:\n",
    "        pickle.dump(clusterData, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "637.1613354682922\n"
     ]
    }
   ],
   "source": [
    "#with this type of parallel processing execution of this cell takes only about 30 minutes for all the intervals! \n",
    "# if not using parallel processing it would be more than 2 hours!\n",
    "intervals = [60,300,600,900,1200,1500,1800,2100,2400,2700,3000,3300,3600,4500,5400,6300,7200]\n",
    "t2 = time.time()\n",
    "results = Parallel(n_jobs=6)(delayed(cluster_records)(interval,savingLoc) for interval in intervals)\n",
    "print(time.time()-t2)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Location Type Identification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can use the files from the previous step to reconstruct the travel diaries. This include to first indentify the event type i.e., `stay` or `pass-by`, then identify the acticity type , i.e., `home`, `work` or `other`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "def locIdentify(interval,address):\n",
    "    t1= time.time()\n",
    "    seeds = range(101,126)\n",
    "    for seed in seeds:\n",
    "        with open('{a}trainingTrip_seed{s}.pickle'.format(a=address,s=seed), 'rb') as handle:\n",
    "            trip = pickle.load(handle)\n",
    "        with open('{a}trainingActivity_seed{s}.pickle'.format(a=address,s=seed), 'rb') as handle:\n",
    "            activity = pickle.load(handle)\n",
    "        with open('{a}trainingHome_seed{s}.pickle'.format(a=address,s=seed), 'rb') as handle:\n",
    "            home = pickle.load(handle)\n",
    "        with open('{a}trainingWork_seed{s}.pickle'.format(a=address,s=seed), 'rb') as handle:\n",
    "            work = pickle.load(handle)\n",
    "        with open('{a}trainingOther_seed{s}.pickle'.format(a=address,s=seed), 'rb') as handle:\n",
    "            other = pickle.load(handle)\n",
    "        prior_activity = len(activity)/(len(activity) + len(trip))\n",
    "        prior_trip = len(trip)//(len(activity) + len(trip))\n",
    "        prior_home = len(home)/(len(home) + len(work) + len(other))\n",
    "        prior_work = len(work)/(len(home) + len(work) + len(other))\n",
    "        prior_other = len(other)/(len(home) + len(work) + len(other))\n",
    "        with open('{a}clusterData_{b}sec.pickle'.format(a=address,b=interval), 'rb') as handle:\n",
    "            clusterData = pickle.load(handle)\n",
    "        identified_clusterData = {}\n",
    "        for i in clusterData.keys():\n",
    "            d = np.array(clusterData[i].duration)\n",
    "            st = np.array(clusterData[i].start.dt.total_seconds())\n",
    "            zone = clusterData[i].mzr_id\n",
    "            ps = prior_activity* gaussian_kde(np.array(pd.to_timedelta(activity.duration).dt.total_seconds())).pdf(\n",
    "                d)* gaussian_kde(np.array(pd.to_timedelta(activity.start).dt.total_seconds())).pdf(st)\n",
    "            pp = prior_activity* gaussian_kde(np.array(pd.to_timedelta(trip.duration).dt.total_seconds())).pdf(\n",
    "                d)* gaussian_kde(np.array(pd.to_timedelta(trip.start).dt.total_seconds())).pdf(st)\n",
    "            logprob_stay = ps/(ps+pp)\n",
    "            identified_clusterData[i] = pd.DataFrame({\"mzr_id\":np.array(zone)[np.where(logprob_stay>=0.5)],\n",
    "                                         'start':np.array(st[np.where(logprob_stay>=0.5)]),\n",
    "                                         'duration': np.array(d[np.where(logprob_stay >= 0.5)]),\n",
    "                                         \"x\":np.array(clusterData[i].x)[np.where(logprob_stay>=0.5)],\n",
    "                                         \"y\":np.array(clusterData[i].y)[np.where(logprob_stay>=0.5)],\n",
    "                                         \"id\":np.array(clusterData[i].id)[np.where(logprob_stay>=0.5)],\n",
    "                                         \"stayProb\":logprob_stay[logprob_stay>=0.5]})\n",
    "            ph = prior_home * gaussian_kde(np.array(pd.to_timedelta(home.duration).dt.total_seconds())).pdf(\n",
    "                identified_clusterData[i].duration) * gaussian_kde(np.array((pd.to_timedelta(\n",
    "                home.start).dt.total_seconds()))).pdf(identified_clusterData[i].start)\n",
    "            pw = prior_work * gaussian_kde(np.array(pd.to_timedelta(work.duration).dt.total_seconds())).pdf(\n",
    "                identified_clusterData[i].duration) * gaussian_kde(np.array(pd.to_timedelta(\n",
    "                work.start).dt.total_seconds())).pdf(\n",
    "                identified_clusterData[i].start)\n",
    "            po = prior_other * gaussian_kde(np.array(pd.to_timedelta(other.duration).dt.total_seconds())).pdf(\n",
    "                identified_clusterData[i].duration) * gaussian_kde(np.array(\n",
    "                pd.to_timedelta(other.start).dt.total_seconds())).pdf(\n",
    "                identified_clusterData[i].start)\n",
    "            identified_clusterData[i][\"homeProb\"] = ph/(ph+pw+po)\n",
    "            identified_clusterData[i][\"workProb\"] = pw / (ph + pw + po)\n",
    "            identified_clusterData[i][\"otherProb\"] = po / (ph + pw + po)\n",
    "            identified_clusterData[i][\"activity\"] = np.argmax(np.vstack((identified_clusterData[i].homeProb,\n",
    "                                                                         identified_clusterData[i].workProb,\n",
    "                                                                         identified_clusterData[i].otherProb)),axis=0)\n",
    "        print(\"seed:  \",seed, \",  interval:   \",interval, \",time:   \",(time.time()-t1)//60, \" min\")\n",
    "        with open('{a}clusterData_identified_{b}sec_seed{ss}.pickle'.format(a=address,b=interval,ss=seed),\n",
    "                  'wb') as handle:\n",
    "            pickle.dump(identified_clusterData, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        with open('{a}clusterData_identifiedStayOnly_{b}sec_seed{ss}.pickle'.format(a=address,b=interval,ss=seed),\n",
    "                  'wb') as handle:\n",
    "            pickle.dump(clusterData, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43135.713884830475\n"
     ]
    }
   ],
   "source": [
    "intervals = [30,60,300,600,900,1200,1500,1800,2100,2400,2700,3000,3300,3600,4500,5400,6300,7200]\n",
    "t2 = time.time()\n",
    "results = Parallel(n_jobs=6)(delayed(locIdentify)(interval,savingLoc) for interval in intervals)\n",
    "print(time.time()-t2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.127824068069458\n"
     ]
    }
   ],
   "source": [
    "t1= time.time()\n",
    "address = savingLoc\n",
    "interval=900\n",
    "seeds = range(101,102)\n",
    "for seed in seeds:\n",
    "    with open('{a}trainingTrip_seed{s}.pickle'.format(a=address,s=seed), 'rb') as handle:\n",
    "        trip = pickle.load(handle)\n",
    "    with open('{a}trainingActivity_seed{s}.pickle'.format(a=address,s=seed), 'rb') as handle:\n",
    "        activity = pickle.load(handle)\n",
    "    with open('{a}trainingHome_seed{s}.pickle'.format(a=address,s=seed), 'rb') as handle:\n",
    "        home = pickle.load(handle)\n",
    "    with open('{a}trainingWork_seed{s}.pickle'.format(a=address,s=seed), 'rb') as handle:\n",
    "        work = pickle.load(handle)\n",
    "    with open('{a}trainingOther_seed{s}.pickle'.format(a=address,s=seed), 'rb') as handle:\n",
    "        other = pickle.load(handle)\n",
    "    prior_activity = len(activity)/(len(activity) + len(trip))\n",
    "    prior_trip = len(trip)//(len(activity) + len(trip))\n",
    "    prior_home = len(home)/(len(home) + len(work) + len(other))\n",
    "    prior_work = len(work)/(len(home) + len(work) + len(other))\n",
    "    prior_other = len(other)/(len(home) + len(work) + len(other))\n",
    "    with open('{a}clusterData_{b}sec.pickle'.format(a=address,b=interval), 'rb') as handle:\n",
    "        clusterData2 = pickle.load(handle)\n",
    "    \n",
    "    clusterData = dict((kk, clusterData2[kk]) for kk in IDD)\n",
    "\n",
    "\n",
    "#         pd.to_timedelta(activity.duration).dt.total_seconds()\n",
    "    identified_clusterData = {}\n",
    "    for i in clusterData.keys():\n",
    "        d = np.array(clusterData[i].duration)\n",
    "        st = np.array(clusterData[i].start.dt.total_seconds())\n",
    "        zone = clusterData[i].mzr_id\n",
    "        ps = prior_activity* gaussian_kde(np.array(pd.to_timedelta(activity.duration).dt.total_seconds())).pdf(\n",
    "            d)* gaussian_kde(np.array(pd.to_timedelta(activity.start).dt.total_seconds())).pdf(st)\n",
    "        pp = prior_activity* gaussian_kde(np.array(pd.to_timedelta(trip.duration).dt.total_seconds())).pdf(\n",
    "            d)* gaussian_kde(np.array(pd.to_timedelta(trip.start).dt.total_seconds())).pdf(st)\n",
    "        logprob_stay = ps/(ps+pp)\n",
    "        identified_clusterData[i] = pd.DataFrame({\"mzr_id\":np.array(zone)[np.where(logprob_stay>=0.5)],\n",
    "                                     'start':np.array(st[np.where(logprob_stay>=0.5)]),\n",
    "                                     'duration': np.array(d[np.where(logprob_stay >= 0.5)]),\n",
    "                                     \"x\":np.array(clusterData[i].x)[np.where(logprob_stay>=0.5)],\n",
    "                                     \"y\":np.array(clusterData[i].y)[np.where(logprob_stay>=0.5)],\n",
    "                                     \"id\":np.array(clusterData[i].id)[np.where(logprob_stay>=0.5)],\n",
    "                                     \"stayProb\":logprob_stay[logprob_stay>=0.5]})\n",
    "        ph = prior_home * gaussian_kde(np.array(pd.to_timedelta(home.duration).dt.total_seconds())).pdf(\n",
    "            identified_clusterData[i].duration) * gaussian_kde(np.array((pd.to_timedelta(\n",
    "            home.start).dt.total_seconds()))).pdf(identified_clusterData[i].start)\n",
    "        pw = prior_work * gaussian_kde(np.array(pd.to_timedelta(work.duration).dt.total_seconds())).pdf(\n",
    "            identified_clusterData[i].duration) * gaussian_kde(np.array(pd.to_timedelta(\n",
    "            work.start).dt.total_seconds())).pdf(\n",
    "            identified_clusterData[i].start)\n",
    "        po = prior_other * gaussian_kde(np.array(pd.to_timedelta(other.duration).dt.total_seconds())).pdf(\n",
    "            identified_clusterData[i].duration) * gaussian_kde(np.array(\n",
    "            pd.to_timedelta(other.start).dt.total_seconds())).pdf(\n",
    "            identified_clusterData[i].start)\n",
    "        identified_clusterData[i][\"homeProb\"] = ph/(ph+pw+po)\n",
    "        identified_clusterData[i][\"workProb\"] = pw / (ph + pw + po)\n",
    "        identified_clusterData[i][\"otherProb\"] = po / (ph + pw + po)\n",
    "        identified_clusterData[i][\"activity\"] = np.argmax(np.vstack((identified_clusterData[i].homeProb,\n",
    "                                                                     identified_clusterData[i].workProb,\n",
    "                                                                     identified_clusterData[i].otherProb)),axis=0)\n",
    "print(time.time()-t1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
