{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f3aedcb2",
   "metadata": {},
   "source": [
    "# Estimation and Analysis of the ground-truth OD matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df9c88e4",
   "metadata": {},
   "source": [
    "#### adding required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "60dc51d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import time\n",
    "import sys\n",
    "# !conda install --yes --prefix {sys.prefix} pyproj\n",
    "# !{sys.executable} -m pip install geopandas\n",
    "from pyproj import Proj, transform\n",
    "from shapely.geometry import Point\n",
    "import geopandas as gpd\n",
    "import random\n",
    "from scipy.stats import gaussian_kde\n",
    "import datetime as dt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "192eea68",
   "metadata": {},
   "source": [
    "#### specifying the saving location "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "c46e1cf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "savingLoc = \"Y:/ZahraEftekhar/phase4/\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "810b8506",
   "metadata": {},
   "source": [
    "#### functions for proximity analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7612e1cb",
   "metadata": {},
   "source": [
    "Here, we create classes and functions to automate the proximity analysis. These analyses include changing the coordinate system, mapping the location coordinates to their associated TAZ (traffic analysis zone)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "7c25437a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#this class change coordinate system and map location coordinates to zones:\n",
    "class LongLat:\n",
    "    def __init__(self, *args):\n",
    "        self.TAZ = 0\n",
    "    def set_location(self, x, y):\n",
    "        from shapely.geometry import Point\n",
    "        self.location = Point(x, y)\n",
    "\n",
    "    def changeCoordSys(self, initial: str = 'epsg:23031', final: str = 'epsg:28992'):\n",
    "        from pyproj import Proj, transform\n",
    "        from shapely.geometry import Point\n",
    "        self.location = Point(transform(Proj(init=initial), Proj(init=final), self.location.x, self.location.y))\n",
    "\n",
    "    def zoneMapping(self, onepolygon, polygonName):\n",
    "        if (onepolygon.contains(self.location)):\n",
    "            self.TAZ = polygonName\n",
    "# this class only reads SHP:\n",
    "class TAZmap:\n",
    "    def __init__(self): pass\n",
    "\n",
    "    def set_map(self, value):\n",
    "        import geopandas as gpd\n",
    "        self.map = gpd.read_file(value).loc[:,['mzr_id','geometry']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "079bdb2b",
   "metadata": {},
   "source": [
    "#### building the zero OD Matrix based on the number of zones in the SHP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "dafad35e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# building the zero OD Matrix based on the number of zones in the SHP\n",
    "map_mzr = TAZmap()\n",
    "map_mzr.set_map(\"{a}amsterdamMezuroZones.shp\".format(a=savingLoc))\n",
    "inputs = map_mzr.map.geometry\n",
    "amsterdamMezuroZones = pd.read_csv(\"{a}amsterdamMezuroZones.CSV\".format(a=savingLoc), usecols=['mzr_id'])\n",
    "tazNames = amsterdamMezuroZones['mzr_id']\n",
    "zoneZero = pd.Series(0)\n",
    "matrixRowColNames = tuple(zoneZero.append(tazNames))\n",
    "odsize=len(matrixRowColNames)\n",
    "del map_mzr\n",
    "ODMatrix_df = pd.DataFrame(np.zeros((odsize, odsize), dtype=np.int32), columns=matrixRowColNames,\n",
    "                           index=matrixRowColNames)  # creating empty OD matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9bd9bc9",
   "metadata": {},
   "source": [
    "## Specifying the OD matrix estimation interval\n",
    "========================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "4f61f168",
   "metadata": {},
   "outputs": [],
   "source": [
    "ODstart = \"06:30:00\"\n",
    "ODend = \"09:30:00\"\n",
    "startTime_OD = pd.to_timedelta(ODstart)\n",
    "endTime_OD = pd.to_timedelta(ODend)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51da44ba",
   "metadata": {},
   "source": [
    "#### Reading the travel diaries to estimate the OD matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "60e03f49",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('{a}1.trueLocExperienced.pickle'.format(a=savingLoc), 'rb') as handle:\n",
    "    itemlistExperienced = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5edbb8c4",
   "metadata": {},
   "source": [
    "======================================================================================================"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67d26d0d",
   "metadata": {},
   "source": [
    "#### estimating the ground-truth OD matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dad8f77",
   "metadata": {},
   "outputs": [],
   "source": [
    "#we time the process\n",
    "startTime = time.time()\n",
    "for ID in itemlistExperienced.keys():#itemlistExperienced.keys() or ['100158'] or ['100048']\n",
    "    activityListExperienced = itemlistExperienced[ID]\n",
    "    activityListExperienced.loc[:,\"end\"].iloc[-1] = pd.to_timedelta(activityListExperienced.loc[:,\"end\"].iloc[-1]\n",
    "                                                                  )+ pd.to_timedelta('24:00:00')\n",
    "    j=1\n",
    "    while j < len(np.arange(len(activityListExperienced))):\n",
    "        if j==len(np.arange(len(activityListExperienced)))-1:\n",
    "            start_time1 = pd.to_timedelta((activityListExperienced.loc[:,\"start\"]).iloc[j])\n",
    "            end_time1 = pd.to_timedelta((activityListExperienced.loc[:,\"end\"]).iloc[j])\n",
    "            start_time2 = pd.to_timedelta((activityListExperienced.loc[:,\"start\"]).iloc[0])+ pd.to_timedelta('24:00:00')\n",
    "            endActivity = end_time1\n",
    "            startNewActivity = start_time2\n",
    "            if pd.to_timedelta('23:59:59')>=pd.to_timedelta(ODstart)>=pd.to_timedelta(start_time1):\n",
    "                startTime_OD = pd.to_timedelta(ODstart)\n",
    "            else:\n",
    "                startTime_OD = pd.to_timedelta(ODstart) + pd.to_timedelta('24:00:00')\n",
    "            if pd.to_timedelta('23:59:59')>= pd.to_timedelta(ODend) >=pd.to_timedelta(start_time1):\n",
    "                endTime_OD =pd.to_timedelta(ODend)\n",
    "            else:\n",
    "                endTime_OD = pd.to_timedelta(ODend)+ pd.to_timedelta('24:00:00')\n",
    "        else:\n",
    "            start_time1 = pd.to_timedelta(activityListExperienced.loc[:,\"start\"].iloc[j])\n",
    "            end_time1 = pd.to_timedelta(activityListExperienced.loc[:,\"end\"].iloc[j])\n",
    "            start_time2 = pd.to_timedelta(activityListExperienced.loc[:,\"start\"].iloc[j+1])\n",
    "            endActivity = end_time1 \n",
    "            startNewActivity = start_time2 \n",
    "        if pd.to_timedelta(start_time1) <= pd.to_timedelta(startTime_OD) < pd.to_timedelta(startNewActivity):\n",
    "            if endTime_OD <= endActivity:\n",
    "                break\n",
    "            else:\n",
    "                while pd.to_timedelta(endTime_OD) > pd.to_timedelta(endActivity):\n",
    "                    point1 = LongLat()\n",
    "                    point1.set_location(x=float(activityListExperienced.loc[:,\"x\"].iloc[j]),\n",
    "                                        y=float(activityListExperienced.loc[:,\"y\"].iloc[j]))#*********ghablan:j-1\n",
    "                    point1.changeCoordSys()\n",
    "                    for k in range(len(tazNames)):\n",
    "                        point1.zoneMapping(inputs[k], tazNames[k])\n",
    "                    origin = point1.TAZ\n",
    "                    point2 = LongLat()\n",
    "                    if j == len(activityListExperienced) - 1:\n",
    "                        point2.set_location(x=float(activityListExperienced.loc[:,\"x\"].iloc[0]),\n",
    "                                            y=float(activityListExperienced.loc[:,\"y\"].iloc[0]))\n",
    "                    else:\n",
    "                        point2.set_location(x=float(activityListExperienced.loc[:,\"x\"].iloc[j+1]),\n",
    "                                        y=float(activityListExperienced.loc[:,\"y\"].iloc[j+1])) #*****ghablan: j\n",
    "                    point2.changeCoordSys()\n",
    "                    for k in range(len(tazNames)):\n",
    "                        point2.zoneMapping(inputs[k], tazNames[k])\n",
    "                    destination = point2.TAZ\n",
    "                    ODMatrix_df[origin][destination] = ODMatrix_df[origin][destination] + 1\n",
    "                    j += 1\n",
    "                    if j > len(np.arange(len(activityListExperienced))) - 1: break\n",
    "                    if j== len(np.arange(len(activityListExperienced)))-1:\n",
    "                        end_time1 = pd.to_timedelta((activityListExperienced.loc[:,\"end\"]).iloc[0])+ pd.to_timedelta('24:00:00')\n",
    "                    else:\n",
    "                        end_time1 = pd.to_timedelta((activityListExperienced.loc[:,\"end\"]).iloc[j])\n",
    "                    endActivity = end_time1\n",
    "                break\n",
    "        j += 1\n",
    "TXTFileName = \"{a}OD({start1}-{start2}_{end1}-{end2}).pickle\".format(a=savingLoc,start1 = ODstart[0:2],\n",
    "                                                                                     start2 = ODstart[3:5],\n",
    "                                                                     end1 = ODend[0:2], end2 = ODend[3:5])\n",
    "with open(TXTFileName, 'wb') as handle:\n",
    "    pickle.dump(ODMatrix_df, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "print((time.time() - startTime)//60,'minutes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "c36c2b1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11948\n"
     ]
    }
   ],
   "source": [
    "print(np.sum(np.sum(ODMatrix_df, axis=0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "758a5886",
   "metadata": {},
   "source": [
    "================================================================================\n",
    "### creating a `dict` file of all the trips that each user has in its travel diaries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "299bb66e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>VEHICLE</th>\n",
       "      <th>activityType</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>start</th>\n",
       "      <th>end</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>work</td>\n",
       "      <td>629393.6676188618</td>\n",
       "      <td>5803949.115843206</td>\n",
       "      <td>06:52:04</td>\n",
       "      <td>16:49:20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>home</td>\n",
       "      <td>632315.3322837545</td>\n",
       "      <td>5817000.086435355</td>\n",
       "      <td>17:10:47</td>\n",
       "      <td>06:29:59</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  VEHICLE activityType                  x                  y     start  \\\n",
       "0       1         work  629393.6676188618  5803949.115843206  06:52:04   \n",
       "0       1         home  632315.3322837545  5817000.086435355  17:10:47   \n",
       "\n",
       "        end  \n",
       "0  16:49:20  \n",
       "0  06:29:59  "
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "itemlistExperienced[\"1\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "270dbc7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\programdata\\anaconda3\\envs\\extractingodfromxml\\lib\\site-packages\\pandas\\core\\indexing.py:670: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._setitem_with_indexer(indexer, value)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0 minutes\n"
     ]
    }
   ],
   "source": [
    "#we time the process\n",
    "startTime = time.time()\n",
    "\n",
    "trips = {}\n",
    "for ID in itemlistExperienced.keys():\n",
    "    activities= itemlistExperienced[ID]\n",
    "    activities.columns = [\"VEHICLE\",\"activityType\",\"x\",\"y\",\"A_start\",\"A_end\"]\n",
    "    activities[\"start\"] = 0\n",
    "    activities[\"duration\"] = 0\n",
    "    \n",
    "    for j in np.arange(0,len(activities)):\n",
    "            activities.loc[:,\"start\"].iloc[j] = pd.to_timedelta(activities.loc[:,\"A_end\"].iloc[j-1])\n",
    "            activities.loc[:,\"duration\"].iloc[j]=pd.to_timedelta(activities.loc[:,\"A_start\"].iloc[j])-pd.to_timedelta(activities.loc[:,\"A_end\"].iloc[j-1])\n",
    "    activities.drop([\"activityType\",\"x\",\"y\",\"A_start\",\"A_end\"],axis=1,inplace=True)\n",
    "    trips[ID] = activities\n",
    "TXTFileName = \"{a}trips.pickle\".format(a=savingLoc)\n",
    "with open(TXTFileName, 'wb') as handle:\n",
    "    pickle.dump(trips, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "print((time.time() - startTime)//60,'minutes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "2ade7447",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>VEHICLE</th>\n",
       "      <th>start</th>\n",
       "      <th>duration</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0 days 06:29:59</td>\n",
       "      <td>0 days 00:22:05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0 days 16:49:20</td>\n",
       "      <td>0 days 00:21:27</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  VEHICLE            start         duration\n",
       "0       1  0 days 06:29:59  0 days 00:22:05\n",
       "0       1  0 days 16:49:20  0 days 00:21:27"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trips[\"1\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e9e9432",
   "metadata": {},
   "source": [
    "### analysis of the ground truth location and activity type identification:\n",
    "Here, we use Bayesian Theory to compute the probability of each even and activity, then infer the category depending on the temporal characteristics of each record (i.e starting time and duration). Remmember, we have to include Mezuro zone id instead of location coordinates to represent the spatial aggregation due to the fact that in empirical data the BTS location is reported, NOT the user's location. This only means that the diagonal of the OD matrix becomes zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7e75e79a",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('{a}1.trueLocExperienced.pickle'.format(a=savingLoc), 'rb') as handle:\n",
    "    activities = pickle.load(handle)\n",
    "for ID in activities.keys():\n",
    "    activities[ID].loc[:,\"end\"].iloc[-1] = pd.to_timedelta(activities[ID].loc[:,\"end\"].iloc[-1]\n",
    "                                                                  )+ pd.to_timedelta('24:00:00')\n",
    "    activities[ID].loc[:,\"duration\"] =  pd.to_timedelta(activities[ID].loc[:,\"end\"])-pd.to_timedelta(activities[ID].loc[:,\"start\"])\n",
    "#this file is the same as `1.trueLocExperienced.pickle` except that it also includes the duration of activities\n",
    "TXTFileName = \"{a}activities.pickle\".format(a=savingLoc)\n",
    "with open(TXTFileName, 'wb') as handle:\n",
    "    pickle.dump(activities, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "df86c453",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"{a}trips.pickle\".format(a=savingLoc),'rb') as handle:\n",
    "    trips = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a12b55fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#generating dataframe of trips and activities for the ease of operations and visualizability\n",
    "activity_df = pd.DataFrame()\n",
    "trip_df = pd.DataFrame()\n",
    "for ID in activities.keys():\n",
    "    activity_df = activity_df.append(activities[ID])\n",
    "    trip_df = trip_df.append(trips[ID])\n",
    "activity_df.reset_index(drop=True,inplace=True)\n",
    "trip_df.reset_index(drop=True,inplace=True)\n",
    "with open('{a}activity_df.pickle'.format(a=savingLoc), 'wb') as handle:\n",
    "    pickle.dump(activity_df, handle, protocol=pickle.HIGHEST_PROTOCOL)    \n",
    "with open('{a}trip_df.pickle'.format(a=savingLoc), 'wb') as handle:\n",
    "    pickle.dump(trip_df, handle, protocol=pickle.HIGHEST_PROTOCOL)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "6acb71d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>VEHICLE</th>\n",
       "      <th>activityType</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>start</th>\n",
       "      <th>end</th>\n",
       "      <th>duration</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>work</td>\n",
       "      <td>629393.6676188618</td>\n",
       "      <td>5803949.115843206</td>\n",
       "      <td>06:52:04</td>\n",
       "      <td>16:49:20</td>\n",
       "      <td>09:57:16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>home</td>\n",
       "      <td>632315.3322837545</td>\n",
       "      <td>5817000.086435355</td>\n",
       "      <td>17:10:47</td>\n",
       "      <td>1 days 06:29:59</td>\n",
       "      <td>13:19:12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10</td>\n",
       "      <td>work</td>\n",
       "      <td>627192.4415982522</td>\n",
       "      <td>5799465.4065392725</td>\n",
       "      <td>06:01:34</td>\n",
       "      <td>15:27:02</td>\n",
       "      <td>09:25:28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10</td>\n",
       "      <td>home</td>\n",
       "      <td>632315.3322837545</td>\n",
       "      <td>5817000.086435355</td>\n",
       "      <td>15:54:40</td>\n",
       "      <td>1 days 05:35:01</td>\n",
       "      <td>13:40:21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>100007</td>\n",
       "      <td>work</td>\n",
       "      <td>634456.1049276257</td>\n",
       "      <td>5793373.482339734</td>\n",
       "      <td>06:32:44</td>\n",
       "      <td>15:59:53</td>\n",
       "      <td>09:27:09</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  VEHICLE activityType                  x                   y     start  \\\n",
       "0       1         work  629393.6676188618   5803949.115843206  06:52:04   \n",
       "1       1         home  632315.3322837545   5817000.086435355  17:10:47   \n",
       "2      10         work  627192.4415982522  5799465.4065392725  06:01:34   \n",
       "3      10         home  632315.3322837545   5817000.086435355  15:54:40   \n",
       "4  100007         work  634456.1049276257   5793373.482339734  06:32:44   \n",
       "\n",
       "               end duration  \n",
       "0         16:49:20 09:57:16  \n",
       "1  1 days 06:29:59 13:19:12  \n",
       "2         15:27:02 09:25:28  \n",
       "3  1 days 05:35:01 13:40:21  \n",
       "4         15:59:53 09:27:09  "
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "activity_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3bd11ba",
   "metadata": {},
   "source": [
    "Now, we generate __training sets__ from trips and activities. This includes the trips and activity of 1% of the users. each training record is the label, duration and starting time of that event/activity. Then we use the entire data as the __test set__ to identify the location/activity category. The location category is either `stay`(activity) or `pass-by`(trip). The activity categories are either `home`, `work` or `other`. In this step we compute the identification accuracy which is the number of correctly identified calss devided by the number of each class. For instance we compute `home` identification accuracy as follows:\n",
    "$a = \\frac{true-positive-`home`}{actual-number-`home`s } $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "c148f71c",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('{a}activity_df.pickle'.format(a=savingLoc), 'rb') as handle:\n",
    "    activities = pickle.load(handle)\n",
    "with open(\"{a}trip_df.pickle\".format(a=savingLoc), 'rb') as handle:\n",
    "    trips = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "24780bbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ZAHRAE~1\\AppData\\Local\\Temp/ipykernel_14136/3358508287.py:52: RuntimeWarning: divide by zero encountered in log\n",
      "  pd.to_timedelta(activities.start)).dt.total_seconds()))))+np.log(((gaussian_kde(\n",
      "C:\\Users\\ZAHRAE~1\\AppData\\Local\\Temp/ipykernel_14136/3358508287.py:64: RuntimeWarning: divide by zero encountered in log\n",
      "  pd.to_timedelta(trips.start)).dt.total_seconds()))))+np.log(((gaussian_kde(\n",
      "C:\\Users\\ZAHRAE~1\\AppData\\Local\\Temp/ipykernel_14136/3358508287.py:80: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  home['home'] = np.log(prior_home)+np.log(((gaussian_kde((\n",
      "C:\\Users\\ZAHRAE~1\\AppData\\Local\\Temp/ipykernel_14136/3358508287.py:86: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  home['work'] = np.log(prior_work)+np.log(((gaussian_kde((\n",
      "C:\\Users\\ZAHRAE~1\\AppData\\Local\\Temp/ipykernel_14136/3358508287.py:94: RuntimeWarning: divide by zero encountered in log\n",
      "  pd.to_timedelta(home.start)).dt.total_seconds()))))+np.log(((gaussian_kde(\n",
      "C:\\Users\\ZAHRAE~1\\AppData\\Local\\Temp/ipykernel_14136/3358508287.py:92: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  home['other'] = np.log(prior_other)+np.log(((gaussian_kde((\n",
      "C:\\Users\\ZAHRAE~1\\AppData\\Local\\Temp/ipykernel_14136/3358508287.py:98: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  home['activity?'] = home[['home','work','other']].idxmax(axis=1)\n",
      "C:\\Users\\ZAHRAE~1\\AppData\\Local\\Temp/ipykernel_14136/3358508287.py:120: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  work['home'] = np.log(prior_home)+np.log(((gaussian_kde((\n",
      "C:\\Users\\ZAHRAE~1\\AppData\\Local\\Temp/ipykernel_14136/3358508287.py:126: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  work['work'] = np.log(prior_work)+np.log(((gaussian_kde((\n",
      "C:\\Users\\ZAHRAE~1\\AppData\\Local\\Temp/ipykernel_14136/3358508287.py:132: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  work['other'] = np.log(prior_other)+np.log(((gaussian_kde((\n",
      "C:\\Users\\ZAHRAE~1\\AppData\\Local\\Temp/ipykernel_14136/3358508287.py:138: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  work['activity?'] = work[['home','work','other']].idxmax(axis=1)\n",
      "C:\\Users\\ZAHRAE~1\\AppData\\Local\\Temp/ipykernel_14136/3358508287.py:142: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  other['home'] = np.log(prior_home)+np.log(((gaussian_kde((\n",
      "C:\\Users\\ZAHRAE~1\\AppData\\Local\\Temp/ipykernel_14136/3358508287.py:148: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  other['work'] = np.log(prior_work)+np.log(((gaussian_kde((\n",
      "C:\\Users\\ZAHRAE~1\\AppData\\Local\\Temp/ipykernel_14136/3358508287.py:154: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  other['other'] = np.log(prior_other)+np.log(((gaussian_kde((\n",
      "C:\\Users\\ZAHRAE~1\\AppData\\Local\\Temp/ipykernel_14136/3358508287.py:160: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  other['activity?'] = other[['home','work','other']].idxmax(axis=1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.0 minutes\n"
     ]
    }
   ],
   "source": [
    "#we time the process\n",
    "startTime = time.time()\n",
    "\n",
    "# _______________test seed based on user sampling _________________________________\n",
    "seedSet = np.arange(101,151)\n",
    "sensitivityTable = pd.DataFrame(index=seedSet)\n",
    "sensitivityTable.index=seedSet\n",
    "for s, seed in enumerate(seedSet):\n",
    "    ids = pd.unique(activities.VEHICLE)\n",
    "    random.seed(seed)\n",
    "    indices = random.sample(range(len(ids)),round(.01*len(ids))) #1% sampling of users\n",
    "    indices = pd.DataFrame(indices, columns=[\"VEHICLE\"])\n",
    "    trainIDs =pd.DataFrame(ids[indices])\n",
    "    trainIDs.columns = [\"VEHICLE\"]\n",
    "    trainingSet_trips = (trainIDs.merge(trips, on=[\"VEHICLE\"], how=\"inner\", sort=True,validate=\"1:m\"))\n",
    "    trainingSet_activities = (trainIDs.merge(activities, on=[\"VEHICLE\"], how=\"inner\", sort=True,validate=\"1:m\"))\n",
    "    with open('{a}/trainingActivity_seed{s}.pickle'.format(a=savingLoc,s=seed), 'wb') as handle:\n",
    "        pickle.dump(trainingSet_activities, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    with open('{a}trainingTrip_seed{s}.pickle'.format(a=savingLoc,s=seed), 'wb') as handle:\n",
    "        pickle.dump(trainingSet_trips, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "\n",
    "    trainingSet_home = trainingSet_activities[trainingSet_activities.activityType == \"home\"]\n",
    "    trainingSet_work = trainingSet_activities[trainingSet_activities.activityType == \"work\"]\n",
    "    trainingSet_other = trainingSet_activities[(trainingSet_activities.activityType != \"home\") & (trainingSet_activities.activityType != \"work\")]\n",
    "    trainingSet_home.to_csv(\"{a}trainingHome_seed{s}.CSV\".format(a=savingLoc,s=seed),index=False,header=True)\n",
    "    trainingSet_work.to_csv(\"{a}trainingWork_seed{s}.CSV\".format(a=savingLoc,s=seed),index=False,header=True)\n",
    "    trainingSet_other.to_csv(\"{a}trainingOther_seed{s}.CSV\".format(a=savingLoc,s=seed),index=False,header=True)\n",
    "\n",
    "    with open('{a}trainingHome_seed{s}.pickle'.format(a=savingLoc,s=seed), 'wb') as handle:\n",
    "        pickle.dump(trainingSet_home, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    with open('{a}trainingWork_seed{s}.pickle'.format(a=savingLoc,s=seed), 'wb') as handle:\n",
    "        pickle.dump(trainingSet_work, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    with open('{a}trainingOther_seed{s}.pickle'.format(a=savingLoc,s=seed), 'wb') as handle:\n",
    "        pickle.dump(trainingSet_other, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    # ________________________________________________________________________________________\n",
    "    # ****************************************************************************************\n",
    "    home = activities[activities.activityType== \"home\"]\n",
    "    work = activities[activities.activityType== \"work\"]\n",
    "    other = activities[(activities.activityType != \"home\") & (activities.activityType != \"work\")]\n",
    "    # ****************************************************************************************\n",
    "    # ________________________ writing data: Trainset _________________________\n",
    "\n",
    "    # ________________________________________________________________\n",
    "\n",
    "    # ___________________________ Probability calculations_________________________________________\n",
    "    #***************************************************************************\n",
    "    prior_activity = len(activities)/(len(activities) + len(trips))\n",
    "    prior_trip =  len(trips)/(len(activities) + len(trips))\n",
    "    activities['activity?'] = np.log(prior_trip)+np.log(((gaussian_kde((\n",
    "        pd.to_timedelta(trainingSet_trips.start)).dt.total_seconds()).pdf((\n",
    "        pd.to_timedelta(activities.start)).dt.total_seconds()))))+np.log(((gaussian_kde(\n",
    "        (pd.to_timedelta(trainingSet_trips.duration)).dt.total_seconds()).pdf((\n",
    "        pd.to_timedelta(activities.duration)).dt.total_seconds()))))<=np.log(prior_activity)+np.log(((gaussian_kde((\n",
    "        pd.to_timedelta(trainingSet_activities.start)).dt.total_seconds()).pdf((\n",
    "        pd.to_timedelta(activities.start)).dt.total_seconds()))))+np.log(((gaussian_kde(\n",
    "        (pd.to_timedelta(trainingSet_activities.duration)).dt.total_seconds()).pdf((\n",
    "        pd.to_timedelta(activities.duration)).dt.total_seconds()))))\n",
    "    sensitivityTable.loc[seed,\"stay accuracy\"] = sum(activities['activity?'])/len(activities)\n",
    "\n",
    "    #***************************************************************************\n",
    "    trips['trip?'] = np.log(prior_trip)+np.log(((gaussian_kde((\n",
    "        pd.to_timedelta(trainingSet_trips.start)).dt.total_seconds()).pdf((\n",
    "        pd.to_timedelta(trips.start)).dt.total_seconds()))))+np.log(((gaussian_kde(\n",
    "        (pd.to_timedelta(trainingSet_trips.duration)).dt.total_seconds()).pdf((\n",
    "        pd.to_timedelta(trips.duration)).dt.total_seconds()))))>=np.log(prior_activity)+np.log(((gaussian_kde((\n",
    "        pd.to_timedelta(trainingSet_activities.start)).dt.total_seconds()).pdf((\n",
    "        pd.to_timedelta(trips.start)).dt.total_seconds()))))+np.log(((gaussian_kde(\n",
    "        (pd.to_timedelta(trainingSet_activities.duration)).dt.total_seconds()).pdf((\n",
    "        pd.to_timedelta(trips.duration)).dt.total_seconds()))))\n",
    "    sensitivityTable.loc[seed,\"pass-by accuracy\"] = sum(trips['trip?'])/len(trips)\n",
    "\n",
    "\n",
    "    #***************************************************************************\n",
    "\n",
    "    prior_home = len(trainingSet_home)/(len(trainingSet_home) + len(trainingSet_work) + len(trainingSet_other))\n",
    "    prior_work =  len(trainingSet_work)/(len(trainingSet_home) + len(trainingSet_work) + len(trainingSet_other))\n",
    "    prior_other = len(trainingSet_other)/(len(trainingSet_home) + len(trainingSet_work) + len(trainingSet_other))\n",
    "\n",
    "    home['home'] = np.log(prior_home)+np.log(((gaussian_kde((\n",
    "        pd.to_timedelta(trainingSet_home.start)).dt.total_seconds()).pdf((\n",
    "        pd.to_timedelta(home.start)).dt.total_seconds()))))+np.log(((gaussian_kde(\n",
    "        (pd.to_timedelta(trainingSet_home.duration)).dt.total_seconds()).pdf((\n",
    "        pd.to_timedelta(home.duration)).dt.total_seconds()))))\n",
    "\n",
    "    home['work'] = np.log(prior_work)+np.log(((gaussian_kde((\n",
    "        pd.to_timedelta(trainingSet_work.start)).dt.total_seconds()).pdf((\n",
    "        pd.to_timedelta(home.start)).dt.total_seconds()))))+np.log(((gaussian_kde(\n",
    "        (pd.to_timedelta(trainingSet_work.duration)).dt.total_seconds()).pdf((\n",
    "        pd.to_timedelta(home.duration)).dt.total_seconds()))))\n",
    "\n",
    "    home['other'] = np.log(prior_other)+np.log(((gaussian_kde((\n",
    "        pd.to_timedelta(trainingSet_other.start)).dt.total_seconds()).pdf((\n",
    "        pd.to_timedelta(home.start)).dt.total_seconds()))))+np.log(((gaussian_kde(\n",
    "        (pd.to_timedelta(trainingSet_other.duration)).dt.total_seconds()).pdf((\n",
    "        pd.to_timedelta(home.duration)).dt.total_seconds()))))\n",
    "\n",
    "    home['activity?'] = home[['home','work','other']].idxmax(axis=1)\n",
    "#     (np.log(prior_home+((gaussian_kde((\n",
    "#         pd.to_timedelta(trainingSet_home.start)).dt.total_seconds()).pdf((\n",
    "#         pd.to_timedelta(home.start)).dt.total_seconds())))+((gaussian_kde(\n",
    "#         (pd.to_timedelta(trainingSet_trips.duration)).dt.total_seconds()).pdf((\n",
    "#         pd.to_timedelta(home.duration)).dt.total_seconds()))))>=np.log(prior_work+((gaussian_kde((\n",
    "#         pd.to_timedelta(trainingSet_work.start)).dt.total_seconds()).pdf((\n",
    "#         pd.to_timedelta(home.start)).dt.total_seconds())))+((gaussian_kde(\n",
    "#         (pd.to_timedelta(trainingSet_work.duration)).dt.total_seconds()).pdf((\n",
    "#         pd.to_timedelta(home.duration)).dt.total_seconds()))))) &(np.log(prior_home+((gaussian_kde((\n",
    "#         pd.to_timedelta(trainingSet_home.start)).dt.total_seconds()).pdf((\n",
    "#         pd.to_timedelta(home.start)).dt.total_seconds())))+((gaussian_kde(\n",
    "#         (pd.to_timedelta(trainingSet_home.duration)).dt.total_seconds()).pdf((\n",
    "#         pd.to_timedelta(home.duration)).dt.total_seconds()))))>=np.log(prior_other+((gaussian_kde((\n",
    "#         pd.to_timedelta(trainingSet_other.start)).dt.total_seconds()).pdf((\n",
    "#         pd.to_timedelta(home.start)).dt.total_seconds())))+((gaussian_kde(\n",
    "#         (pd.to_timedelta(trainingSet_other.duration)).dt.total_seconds()).pdf((\n",
    "#         pd.to_timedelta(home.duration)).dt.total_seconds())))))\n",
    "    sensitivityTable.loc[seed,\"home accuracy\"] = sum(home['activity?']==\"home\")/len(home)\n",
    "\n",
    "\n",
    "#***************************************************************************\n",
    "    work['home'] = np.log(prior_home)+np.log(((gaussian_kde((\n",
    "        pd.to_timedelta(trainingSet_home.start)).dt.total_seconds()).pdf((\n",
    "        pd.to_timedelta(work.start)).dt.total_seconds()))))+np.log(((gaussian_kde(\n",
    "        (pd.to_timedelta(trainingSet_home.duration)).dt.total_seconds()).pdf((\n",
    "        pd.to_timedelta(work.duration)).dt.total_seconds()))))\n",
    "\n",
    "    work['work'] = np.log(prior_work)+np.log(((gaussian_kde((\n",
    "        pd.to_timedelta(trainingSet_work.start)).dt.total_seconds()).pdf((\n",
    "        pd.to_timedelta(work.start)).dt.total_seconds()))))+np.log(((gaussian_kde(\n",
    "        (pd.to_timedelta(trainingSet_work.duration)).dt.total_seconds()).pdf((\n",
    "        pd.to_timedelta(work.duration)).dt.total_seconds()))))\n",
    "\n",
    "    work['other'] = np.log(prior_other)+np.log(((gaussian_kde((\n",
    "        pd.to_timedelta(trainingSet_other.start)).dt.total_seconds()).pdf((\n",
    "        pd.to_timedelta(work.start)).dt.total_seconds()))))+np.log(((gaussian_kde(\n",
    "        (pd.to_timedelta(trainingSet_other.duration)).dt.total_seconds()).pdf((\n",
    "        pd.to_timedelta(work.duration)).dt.total_seconds()))))\n",
    "\n",
    "    work['activity?'] = work[['home','work','other']].idxmax(axis=1)\n",
    "    sensitivityTable.loc[seed,\"work accuracy\"] = sum(work['activity?']==\"work\")/len(work)\n",
    "\n",
    "#***************************************************************************\n",
    "    other['home'] = np.log(prior_home)+np.log(((gaussian_kde((\n",
    "        pd.to_timedelta(trainingSet_home.start)).dt.total_seconds()).pdf((\n",
    "        pd.to_timedelta(other.start)).dt.total_seconds()))))+np.log(((gaussian_kde(\n",
    "        (pd.to_timedelta(trainingSet_home.duration)).dt.total_seconds()).pdf((\n",
    "        pd.to_timedelta(other.duration)).dt.total_seconds()))))\n",
    "\n",
    "    other['work'] = np.log(prior_work)+np.log(((gaussian_kde((\n",
    "        pd.to_timedelta(trainingSet_work.start)).dt.total_seconds()).pdf((\n",
    "        pd.to_timedelta(other.start)).dt.total_seconds()))))+np.log(((gaussian_kde(\n",
    "        (pd.to_timedelta(trainingSet_work.duration)).dt.total_seconds()).pdf((\n",
    "        pd.to_timedelta(other.duration)).dt.total_seconds()))))\n",
    "\n",
    "    other['other'] = np.log(prior_other)+np.log(((gaussian_kde((\n",
    "        pd.to_timedelta(trainingSet_other.start)).dt.total_seconds()).pdf((\n",
    "        pd.to_timedelta(other.start)).dt.total_seconds()))))+np.log(((gaussian_kde(\n",
    "        (pd.to_timedelta(trainingSet_other.duration)).dt.total_seconds()).pdf((\n",
    "        pd.to_timedelta(other.duration)).dt.total_seconds()))))\n",
    "\n",
    "    other['activity?'] = other[['home','work','other']].idxmax(axis=1)\n",
    "    sensitivityTable.loc[seed,\"other accuracy\"] = sum(other['activity?']==\"other\")/len(other)\n",
    "\n",
    "#     ***************************************************************************\n",
    "    home.to_csv(\"{a}home_seed{s}.CSV\".format(a=savingLoc,s=seed),index=False,header=True)\n",
    "    work.to_csv(\"{a}work_seed{s}.CSV\".format(a=savingLoc,s=seed),index=False,header=True)\n",
    "    other.to_csv(\"{a}other_seed{s}.CSV\".format(a=savingLoc,s=seed),index=False,header=True)\n",
    "\n",
    "    with open('{a}home_seed{s}.pickle'.format(a=savingLoc,s=seed), 'wb') as handle:\n",
    "        pickle.dump(home, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    with open('{a}work_seed{s}.pickle'.format(a=savingLoc,s=seed), 'wb') as handle:\n",
    "        pickle.dump(work, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    with open('{a}other_seed{s}.pickle'.format(a=savingLoc,s=seed), 'wb') as handle:\n",
    "        pickle.dump(other, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "sensitivityTable.to_excel(\"{a}userSampling_onePercentLocationDetectionSensitivity.xlsx\".format(a=savingLoc), header=True,\n",
    "                          index=True)\n",
    "print((time.time() - startTime)//60,'minutes')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "346f0bc4",
   "metadata": {},
   "source": [
    "# Appendix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00d3b3f2",
   "metadata": {},
   "source": [
    "#### Generating spatial training sets with different proportionality to the entire data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "7d0233c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\zahraeftekhar\\.conda\\envs\\ODmatrices\\lib\\site-packages\\pyproj\\crs\\crs.py:53: FutureWarning: '+init=<authority>:<code>' syntax is deprecated. '<authority>:<code>' is the preferred initialization method. When making the change, be mindful of axis order changes: https://pyproj4.github.io/pyproj/stable/gotchas.html#axis-order-changes-in-proj-6\n",
      "  return _prepare_from_string(\" \".join(pjargs))\n",
      "C:\\Users\\zahraeftekhar\\.conda\\envs\\ODmatrices\\lib\\site-packages\\pyproj\\crs\\crs.py:294: FutureWarning: '+init=<authority>:<code>' syntax is deprecated. '<authority>:<code>' is the preferred initialization method. When making the change, be mindful of axis order changes: https://pyproj4.github.io/pyproj/stable/gotchas.html#axis-order-changes-in-proj-6\n",
      "  projstring = _prepare_from_string(\" \".join((projstring, projkwargs)))\n",
      "C:\\Users\\zahraeftekhar\\.conda\\envs\\ODmatrices\\lib\\site-packages\\pyproj\\crs\\crs.py:53: FutureWarning: '+init=<authority>:<code>' syntax is deprecated. '<authority>:<code>' is the preferred initialization method. When making the change, be mindful of axis order changes: https://pyproj4.github.io/pyproj/stable/gotchas.html#axis-order-changes-in-proj-6\n",
      "  return _prepare_from_string(\" \".join(pjargs))\n",
      "C:\\Users\\zahraeftekhar\\.conda\\envs\\ODmatrices\\lib\\site-packages\\pyproj\\crs\\crs.py:294: FutureWarning: '+init=<authority>:<code>' syntax is deprecated. '<authority>:<code>' is the preferred initialization method. When making the change, be mindful of axis order changes: https://pyproj4.github.io/pyproj/stable/gotchas.html#axis-order-changes-in-proj-6\n",
      "  projstring = _prepare_from_string(\" \".join((projstring, projkwargs)))\n",
      "C:\\Users\\ZAHRAE~1\\AppData\\Local\\Temp/ipykernel_14136/3224500027.py:61: DeprecationWarning: This function is deprecated. See: https://pyproj4.github.io/pyproj/stable/gotchas.html#upgrading-to-pyproj-2-from-pyproj-1\n",
      "  for pt in itransform(p1,p2,points, always_xy=True):\n"
     ]
    }
   ],
   "source": [
    "from pyproj import Proj, itransform\n",
    "\n",
    "ids = pd.unique(activities.VEHICLE)\n",
    "random.seed(101)\n",
    "prop=0.10\n",
    "indices = random.sample(range(len(ids)),round(prop*len(ids))) #1% sampling of users\n",
    "indices = pd.DataFrame(indices, columns=[\"VEHICLE\"])\n",
    "trainIDs =pd.DataFrame(ids[indices])\n",
    "trainIDs.columns = [\"VEHICLE\"]\n",
    "\n",
    "# trainingSet_trips = (trainIDs.merge(trips, on=[\"VEHICLE\"], how=\"inner\", sort=True,validate=\"1:m\"))\n",
    "trainingSet_activities = (trainIDs.merge(activities, on=[\"VEHICLE\"], how=\"inner\", sort=True,validate=\"1:m\"))\n",
    "\n",
    "# building the zero OD Matrix based on the number of zones in the SHP\n",
    "map_mzr = TAZmap()\n",
    "map_mzr.set_map(\"{a}amsterdamMezuroZones.shp\".format(a=savingLoc))\n",
    "inputs = map_mzr.map.geometry\n",
    "map_mzr.map\n",
    "ids = pd.unique(activities.VEHICLE)\n",
    "random.seed(101)\n",
    "prop=0.10\n",
    "indices = random.sample(range(len(ids)),round(prop*len(ids))) #1% sampling of users\n",
    "indices = pd.DataFrame(indices, columns=[\"VEHICLE\"])\n",
    "trainIDs =pd.DataFrame(ids[indices])\n",
    "trainIDs.columns = [\"VEHICLE\"]\n",
    "trainingSet_trips = (trainIDs.merge(trips, on=[\"VEHICLE\"], how=\"inner\", sort=True,validate=\"1:m\"))\n",
    "trainingSet_activities = (trainIDs.merge(activities, on=[\"VEHICLE\"], how=\"inner\", sort=True,validate=\"1:m\"))\n",
    "points = list(zip([float(num) for num in (trainingSet_activities.x.values)],[float(num) for num in (trainingSet_activities.y.values)]))\n",
    "\n",
    "# projection 1:\n",
    "p1 = Proj(init='epsg:23031')\n",
    "# projection 2: \n",
    "p2 = Proj(init='epsg:28992')\n",
    "ptsX = []\n",
    "ptsY = []\n",
    "for pt in itransform(p1,p2,points, always_xy=True): \n",
    "    from shapely.geometry import Point\n",
    "    ptsX = ptsX + [pt[0]]\n",
    "    ptsY = ptsY + [pt[1]]\n",
    "\n",
    "    \n",
    "trainingSet_activities = gpd.GeoDataFrame(trainingSet_activities, geometry=gpd.points_from_xy(ptsX,ptsY))\n",
    "trainingSet_activities = trainingSet_activities.set_crs(epsg=28992)\n",
    "trainingSet_activities = gpd.sjoin(trainingSet_activities, map_mzr.map,how = \"left\")\n",
    "trainingSet_activities.mzr_id = (np.nan_to_num((trainingSet_activities.mzr_id)))\n",
    "trainingSet_activities.drop(['index_right'],axis=1,inplace=True)\n",
    "\n",
    "\n",
    "trainingSet_home = trainingSet_activities[trainingSet_activities.activityType == \"home\"]\n",
    "trainingSet_work = trainingSet_activities[trainingSet_activities.activityType == \"work\"]\n",
    "trainingSet_other = trainingSet_activities[(trainingSet_activities.activityType != \"home\") & (trainingSet_activities.activityType != \"work\")]\n",
    "trainingSet_home.to_csv(\"{a}trainingHome_prop{s}.CSV\".format(a=savingLoc,s=prop),index=False,header=True)\n",
    "trainingSet_work.to_csv(\"{a}trainingWork_prop{s}.CSV\".format(a=savingLoc,s=prop),index=False,header=True)\n",
    "trainingSet_other.to_csv(\"{a}trainingOther_prop{s}.CSV\".format(a=savingLoc,s=prop),index=False,header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "9ece6887",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>VEHICLE</th>\n",
       "      <th>activityType</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>start</th>\n",
       "      <th>end</th>\n",
       "      <th>duration</th>\n",
       "      <th>geometry</th>\n",
       "      <th>mzr_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10001</td>\n",
       "      <td>home</td>\n",
       "      <td>625308.5056096063</td>\n",
       "      <td>5792622.8888017945</td>\n",
       "      <td>17:00:03</td>\n",
       "      <td>1 days 06:57:56</td>\n",
       "      <td>0 days 13:57:53</td>\n",
       "      <td>POINT (117306.649 475676.791)</td>\n",
       "      <td>7831.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>100089</td>\n",
       "      <td>home</td>\n",
       "      <td>630028.7167090132</td>\n",
       "      <td>5802955.596728954</td>\n",
       "      <td>22:27:16</td>\n",
       "      <td>1 days 20:20:51</td>\n",
       "      <td>0 days 21:53:35</td>\n",
       "      <td>POINT (122365.447 485849.465)</td>\n",
       "      <td>7514.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>100294</td>\n",
       "      <td>home</td>\n",
       "      <td>630543.4389452781</td>\n",
       "      <td>5803946.491865685</td>\n",
       "      <td>21:53:42</td>\n",
       "      <td>1 days 17:23:16</td>\n",
       "      <td>0 days 19:29:34</td>\n",
       "      <td>POINT (122912.629 486822.948)</td>\n",
       "      <td>7510.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>10052</td>\n",
       "      <td>home</td>\n",
       "      <td>625308.5056096063</td>\n",
       "      <td>5792622.8888017945</td>\n",
       "      <td>15:01:57</td>\n",
       "      <td>1 days 08:14:07</td>\n",
       "      <td>0 days 17:12:10</td>\n",
       "      <td>POINT (117306.649 475676.791)</td>\n",
       "      <td>7831.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>100572</td>\n",
       "      <td>home</td>\n",
       "      <td>629997.9602208312</td>\n",
       "      <td>5803597.994578412</td>\n",
       "      <td>11:03:41</td>\n",
       "      <td>1 days 09:09:00</td>\n",
       "      <td>0 days 22:05:19</td>\n",
       "      <td>POINT (122355.893 486492.597)</td>\n",
       "      <td>7510.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5817</th>\n",
       "      <td>99721</td>\n",
       "      <td>home</td>\n",
       "      <td>628959.7643920127</td>\n",
       "      <td>5803554.357180867</td>\n",
       "      <td>16:11:41</td>\n",
       "      <td>1 days 06:29:18</td>\n",
       "      <td>0 days 14:17:37</td>\n",
       "      <td>POINT (121316.710 486483.224)</td>\n",
       "      <td>7510.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5819</th>\n",
       "      <td>9974</td>\n",
       "      <td>home</td>\n",
       "      <td>625308.5056096063</td>\n",
       "      <td>5792622.8888017945</td>\n",
       "      <td>17:10:09</td>\n",
       "      <td>1 days 07:11:11</td>\n",
       "      <td>0 days 14:01:02</td>\n",
       "      <td>POINT (117306.649 475676.791)</td>\n",
       "      <td>7831.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5822</th>\n",
       "      <td>99799</td>\n",
       "      <td>home</td>\n",
       "      <td>628774.5976465159</td>\n",
       "      <td>5802753.050154505</td>\n",
       "      <td>21:29:21</td>\n",
       "      <td>1 days 14:59:22</td>\n",
       "      <td>0 days 17:30:01</td>\n",
       "      <td>POINT (121105.193 485688.371)</td>\n",
       "      <td>7513.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5824</th>\n",
       "      <td>99876</td>\n",
       "      <td>home</td>\n",
       "      <td>628959.7643920127</td>\n",
       "      <td>5803554.357180867</td>\n",
       "      <td>17:46:42</td>\n",
       "      <td>19:34:52</td>\n",
       "      <td>0 days 01:48:10</td>\n",
       "      <td>POINT (121316.710 486483.224)</td>\n",
       "      <td>7510.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5826</th>\n",
       "      <td>99876</td>\n",
       "      <td>home</td>\n",
       "      <td>628959.7643920127</td>\n",
       "      <td>5803554.357180867</td>\n",
       "      <td>20:11:47</td>\n",
       "      <td>1 days 06:17:54</td>\n",
       "      <td>0 days 10:06:07</td>\n",
       "      <td>POINT (121316.710 486483.224)</td>\n",
       "      <td>7510.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2718 rows  9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     VEHICLE activityType                  x                   y     start  \\\n",
       "1      10001         home  625308.5056096063  5792622.8888017945  17:00:03   \n",
       "3     100089         home  630028.7167090132   5802955.596728954  22:27:16   \n",
       "5     100294         home  630543.4389452781   5803946.491865685  21:53:42   \n",
       "11     10052         home  625308.5056096063  5792622.8888017945  15:01:57   \n",
       "13    100572         home  629997.9602208312   5803597.994578412  11:03:41   \n",
       "...      ...          ...                ...                 ...       ...   \n",
       "5817   99721         home  628959.7643920127   5803554.357180867  16:11:41   \n",
       "5819    9974         home  625308.5056096063  5792622.8888017945  17:10:09   \n",
       "5822   99799         home  628774.5976465159   5802753.050154505  21:29:21   \n",
       "5824   99876         home  628959.7643920127   5803554.357180867  17:46:42   \n",
       "5826   99876         home  628959.7643920127   5803554.357180867  20:11:47   \n",
       "\n",
       "                  end        duration                       geometry  mzr_id  \n",
       "1     1 days 06:57:56 0 days 13:57:53  POINT (117306.649 475676.791)  7831.0  \n",
       "3     1 days 20:20:51 0 days 21:53:35  POINT (122365.447 485849.465)  7514.0  \n",
       "5     1 days 17:23:16 0 days 19:29:34  POINT (122912.629 486822.948)  7510.0  \n",
       "11    1 days 08:14:07 0 days 17:12:10  POINT (117306.649 475676.791)  7831.0  \n",
       "13    1 days 09:09:00 0 days 22:05:19  POINT (122355.893 486492.597)  7510.0  \n",
       "...               ...             ...                            ...     ...  \n",
       "5817  1 days 06:29:18 0 days 14:17:37  POINT (121316.710 486483.224)  7510.0  \n",
       "5819  1 days 07:11:11 0 days 14:01:02  POINT (117306.649 475676.791)  7831.0  \n",
       "5822  1 days 14:59:22 0 days 17:30:01  POINT (121105.193 485688.371)  7513.0  \n",
       "5824         19:34:52 0 days 01:48:10  POINT (121316.710 486483.224)  7510.0  \n",
       "5826  1 days 06:17:54 0 days 10:06:07  POINT (121316.710 486483.224)  7510.0  \n",
       "\n",
       "[2718 rows x 9 columns]"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainingSet_home"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
