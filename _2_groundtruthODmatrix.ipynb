{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f3aedcb2",
   "metadata": {},
   "source": [
    "# Estimation and Analysis of the ground-truth OD matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df9c88e4",
   "metadata": {},
   "source": [
    "#### adding required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "60dc51d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import time\n",
    "import sys\n",
    "# !conda install --yes --prefix {sys.prefix} pyproj\n",
    "# !{sys.executable} -m pip install geopandas\n",
    "from pyproj import Proj, transform\n",
    "from shapely.geometry import Point\n",
    "import geopandas as gpd\n",
    "import random\n",
    "from scipy.stats import gaussian_kde\n",
    "import datetime as dt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "192eea68",
   "metadata": {},
   "source": [
    "#### specifying the saving location "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c46e1cf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "savingLoc = \"Y:/ZahraEftekhar/phase4/\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "810b8506",
   "metadata": {},
   "source": [
    "#### functions for proximity analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7612e1cb",
   "metadata": {},
   "source": [
    "Here, we create classes and functions to automate the proximity analysis. These analyses include changing the coordinate system, mapping the location coordinates to their associated TAZ (traffic analysis zone)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7c25437a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#this class change coordinate system and map location coordinates to zones:\n",
    "class LongLat:\n",
    "    def __init__(self, *args):\n",
    "        self.TAZ = 0\n",
    "    def set_location(self, x, y):\n",
    "        from shapely.geometry import Point\n",
    "        self.location = Point(x, y)\n",
    "\n",
    "    def changeCoordSys(self, initial: str = 'epsg:23031', final: str = 'epsg:28992'):\n",
    "        from pyproj import Proj, transform\n",
    "        from shapely.geometry import Point\n",
    "        self.location = Point(transform(Proj(init=initial), Proj(init=final), self.location.x, self.location.y))\n",
    "\n",
    "    def zoneMapping(self, onepolygon, polygonName):\n",
    "        if (onepolygon.contains(self.location)):\n",
    "            self.TAZ = polygonName\n",
    "# this class only reads SHP:\n",
    "class TAZmap:\n",
    "    def __init__(self): pass\n",
    "\n",
    "    def set_map(self, value):\n",
    "        import geopandas as gpd\n",
    "        self.map = gpd.read_file(value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "079bdb2b",
   "metadata": {},
   "source": [
    "#### building the zero OD Matrix based on the number of zones in the SHP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dafad35e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# building the zero OD Matrix based on the number of zones in the SHP\n",
    "map_mzr = TAZmap()\n",
    "map_mzr.set_map(\"{a}amsterdamMezuroZones.shp\".format(a=savingLoc))\n",
    "inputs = map_mzr.map.geometry\n",
    "amsterdamMezuroZones = pd.read_csv(\"{a}amsterdamMezuroZones.CSV\".format(a=savingLoc), usecols=['mzr_id'])\n",
    "tazNames = amsterdamMezuroZones['mzr_id']\n",
    "zoneZero = pd.Series(0)\n",
    "matrixRowColNames = tuple(zoneZero.append(tazNames))\n",
    "odsize=len(matrixRowColNames)\n",
    "del map_mzr\n",
    "ODMatrix_df = pd.DataFrame(np.zeros((odsize, odsize), dtype=np.int32), columns=matrixRowColNames,\n",
    "                           index=matrixRowColNames)  # creating empty OD matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9bd9bc9",
   "metadata": {},
   "source": [
    "## Specifying the OD matrix estimation interval\n",
    "========================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "4f61f168",
   "metadata": {},
   "outputs": [],
   "source": [
    "ODstart = \"06:30:00\"\n",
    "ODend = \"09:30:00\"\n",
    "startTime_OD = pd.to_timedelta(ODstart)\n",
    "endTime_OD = pd.to_timedelta(ODend)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51da44ba",
   "metadata": {},
   "source": [
    "#### Reading the travel diaries to estimate the OD matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "60e03f49",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('{a}1.trueLocExperienced.pickle'.format(a=savingLoc), 'rb') as handle:\n",
    "    itemlistExperienced = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5edbb8c4",
   "metadata": {},
   "source": [
    "======================================================================================================"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67d26d0d",
   "metadata": {},
   "source": [
    "#### estimating the ground-truth OD matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we time the process\n",
    "startTime = time.time()\n",
    "for ID in itemlistExperienced.keys():#itemlistExperienced.keys() or ['100158'] or ['100048']\n",
    "    activityListExperienced = itemlistExperienced[ID]\n",
    "    activityListExperienced.loc[:,\"end\"].iloc[-1] = pd.to_timedelta(activityListExperienced.loc[:,\"end\"].iloc[-1]\n",
    "                                                                  )+ pd.to_timedelta('24:00:00')\n",
    "    j=1\n",
    "    while j < len(np.arange(len(activityListExperienced))):\n",
    "        if j==len(np.arange(len(activityListExperienced)))-1:\n",
    "            start_time1 = pd.to_timedelta((activityListExperienced.loc[:,\"start\"]).iloc[j])\n",
    "            end_time1 = pd.to_timedelta((activityListExperienced.loc[:,\"end\"]).iloc[j])\n",
    "            start_time2 = pd.to_timedelta((activityListExperienced.loc[:,\"start\"]).iloc[0])+ pd.to_timedelta('24:00:00')\n",
    "            endActivity = end_time1\n",
    "            startNewActivity = start_time2\n",
    "            if pd.to_timedelta('23:59:59')>=pd.to_timedelta(ODstart)>=pd.to_timedelta(start_time1):\n",
    "                startTime_OD = pd.to_timedelta(ODstart)\n",
    "            else:\n",
    "                startTime_OD = pd.to_timedelta(ODstart) + pd.to_timedelta('24:00:00')\n",
    "            if pd.to_timedelta('23:59:59')>= pd.to_timedelta(ODend) >=pd.to_timedelta(start_time1):\n",
    "                endTime_OD =pd.to_timedelta(ODend)\n",
    "            else:\n",
    "                endTime_OD = pd.to_timedelta(ODend)+ pd.to_timedelta('24:00:00')\n",
    "        else:\n",
    "            start_time1 = pd.to_timedelta(activityListExperienced.loc[:,\"start\"].iloc[j])\n",
    "            end_time1 = pd.to_timedelta(activityListExperienced.loc[:,\"end\"].iloc[j])\n",
    "            start_time2 = pd.to_timedelta(activityListExperienced.loc[:,\"start\"].iloc[j+1])\n",
    "            endActivity = end_time1 \n",
    "            startNewActivity = start_time2 \n",
    "        if pd.to_timedelta(start_time1) <= pd.to_timedelta(startTime_OD) < pd.to_timedelta(startNewActivity):\n",
    "            if endTime_OD <= endActivity:\n",
    "                break\n",
    "            else:\n",
    "                while pd.to_timedelta(endTime_OD) > pd.to_timedelta(endActivity):\n",
    "                    point1 = LongLat()\n",
    "                    point1.set_location(x=float(activityListExperienced.loc[:,\"x\"].iloc[j]),\n",
    "                                        y=float(activityListExperienced.loc[:,\"y\"].iloc[j]))#*********ghablan:j-1\n",
    "                    point1.changeCoordSys()\n",
    "                    for k in range(len(tazNames)):\n",
    "                        point1.zoneMapping(inputs[k], tazNames[k])\n",
    "                    origin = point1.TAZ\n",
    "                    point2 = LongLat()\n",
    "                    if j == len(activityListExperienced) - 1:\n",
    "                        point2.set_location(x=float(activityListExperienced.loc[:,\"x\"].iloc[0]),\n",
    "                                            y=float(activityListExperienced.loc[:,\"y\"].iloc[0]))\n",
    "                    else:\n",
    "                        point2.set_location(x=float(activityListExperienced.loc[:,\"x\"].iloc[j+1]),\n",
    "                                        y=float(activityListExperienced.loc[:,\"y\"].iloc[j+1])) #*****ghablan: j\n",
    "                    point2.changeCoordSys()\n",
    "                    for k in range(len(tazNames)):\n",
    "                        point2.zoneMapping(inputs[k], tazNames[k])\n",
    "                    destination = point2.TAZ\n",
    "                    ODMatrix_df[origin][destination] = ODMatrix_df[origin][destination] + 1\n",
    "                    j += 1\n",
    "                    if j > len(np.arange(len(activityListExperienced))) - 1: break\n",
    "                    if j== len(np.arange(len(activityListExperienced)))-1:\n",
    "                        end_time1 = pd.to_timedelta((activityListExperienced.loc[:,\"end\"]).iloc[0])+ pd.to_timedelta('24:00:00')\n",
    "                    else:\n",
    "                        end_time1 = pd.to_timedelta((activityListExperienced.loc[:,\"end\"]).iloc[j])\n",
    "                    endActivity = end_time1\n",
    "                break\n",
    "        j += 1\n",
    "TXTFileName = \"{a}OD({start1}-{start2}_{end1}-{end2}).pickle\".format(a=savingLoc,start1 = ODstart[0:2],\n",
    "                                                                                     start2 = ODstart[3:5],\n",
    "                                                                     end1 = ODend[0:2], end2 = ODend[3:5])\n",
    "with open(TXTFileName, 'wb') as handle:\n",
    "    pickle.dump(ODMatrix_df, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "print((time.time() - startTime)//60,'minutes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "c36c2b1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11948\n"
     ]
    }
   ],
   "source": [
    "print(np.sum(np.sum(ODMatrix_df, axis=0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "================================================================================\n",
    "### creating a `dict` file of all the trips that each user has in its travel diaries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>VEHICLE</th>\n",
       "      <th>activityType</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>start</th>\n",
       "      <th>end</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>work</td>\n",
       "      <td>629393.6676188618</td>\n",
       "      <td>5803949.115843206</td>\n",
       "      <td>06:52:04</td>\n",
       "      <td>16:49:20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>home</td>\n",
       "      <td>632315.3322837545</td>\n",
       "      <td>5817000.086435355</td>\n",
       "      <td>17:10:47</td>\n",
       "      <td>06:29:59</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  VEHICLE activityType                  x                  y     start  \\\n",
       "0       1         work  629393.6676188618  5803949.115843206  06:52:04   \n",
       "0       1         home  632315.3322837545  5817000.086435355  17:10:47   \n",
       "\n",
       "        end  \n",
       "0  16:49:20  \n",
       "0  06:29:59  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "itemlistExperienced[\"1\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\programdata\\anaconda3\\envs\\extractingodfromxml\\lib\\site-packages\\pandas\\core\\indexing.py:670: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._setitem_with_indexer(indexer, value)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0 minutes\n"
     ]
    }
   ],
   "source": [
    "#we time the process\n",
    "startTime = time.time()\n",
    "\n",
    "trips = {}\n",
    "for ID in itemlistExperienced.keys():\n",
    "    activities= itemlistExperienced[ID]\n",
    "    activities.columns = [\"VEHICLE\",\"activityType\",\"x\",\"y\",\"A_start\",\"A_end\"]\n",
    "    activities[\"start\"] = 0\n",
    "    activities[\"duration\"] = 0\n",
    "    \n",
    "    for j in np.arange(0,len(activities)):\n",
    "            activities.loc[:,\"start\"].iloc[j] = pd.to_timedelta(activities.loc[:,\"A_end\"].iloc[j-1])\n",
    "            activities.loc[:,\"duration\"].iloc[j]=pd.to_timedelta(activities.loc[:,\"A_start\"].iloc[j])-pd.to_timedelta(activities.loc[:,\"A_end\"].iloc[j-1])\n",
    "    activities.drop([\"activityType\",\"x\",\"y\",\"A_start\",\"A_end\"],axis=1,inplace=True)\n",
    "    trips[ID] = activities\n",
    "TXTFileName = \"{a}trips.pickle\".format(a=savingLoc)\n",
    "with open(TXTFileName, 'wb') as handle:\n",
    "    pickle.dump(trips, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "print((time.time() - startTime)//60,'minutes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>VEHICLE</th>\n",
       "      <th>start</th>\n",
       "      <th>duration</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0 days 06:29:59</td>\n",
       "      <td>0 days 00:22:05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0 days 16:49:20</td>\n",
       "      <td>0 days 00:21:27</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  VEHICLE            start         duration\n",
       "0       1  0 days 06:29:59  0 days 00:22:05\n",
       "0       1  0 days 16:49:20  0 days 00:21:27"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trips[\"1\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### analysis of the ground truth location and activity type identification:\n",
    "Here, we use Bayesian Theory to compute the probability of each even and activity, then infer the category depending on the temporal characteristics of each record (i.e starting time and duration). Remmember, we have to include Mezuro zone id instead of location coordinates to represent the spatial aggregation due to the fact that in empirical data the BTS location is reported, NOT the user's location. This only means that the diagonal of the OD matrix becomes zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('{a}1.trueLocExperienced.pickle'.format(a=savingLoc), 'rb') as handle:\n",
    "    activities = pickle.load(handle)\n",
    "for ID in activities.keys():\n",
    "    activities[ID].loc[:,\"end\"].iloc[-1] = pd.to_timedelta(activities[ID].loc[:,\"end\"].iloc[-1]\n",
    "                                                                  )+ pd.to_timedelta('24:00:00')\n",
    "    activities[ID].loc[:,\"duration\"] =  pd.to_timedelta(activities[ID].loc[:,\"end\"])-pd.to_timedelta(activities[ID].loc[:,\"start\"])\n",
    "#this file is the same as `1.trueLocExperienced.pickle` except that it also includes the duration of activities\n",
    "TXTFileName = \"{a}activities.pickle\".format(a=savingLoc)\n",
    "with open(TXTFileName, 'wb') as handle:\n",
    "    pickle.dump(activities, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"{a}trips.pickle\".format(a=savingLoc),'rb') as handle:\n",
    "    trips = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generating dataframe of trips and activities for the ease of operations and visualizability\n",
    "activity_df = pd.DataFrame()\n",
    "trip_df = pd.DataFrame()\n",
    "for ID in activities.keys():\n",
    "    activity_df = activity_df.append(activities[ID])\n",
    "    trip_df = trip_df.append(trips[ID])\n",
    "activity_df.reset_index(drop=True,inplace=True)\n",
    "trip_df.reset_index(drop=True,inplace=True)\n",
    "with open('{a}activity_df.pickle'.format(a=savingLoc), 'wb') as handle:\n",
    "    pickle.dump(activity_df, handle, protocol=pickle.HIGHEST_PROTOCOL)    \n",
    "with open('{a}trip_df.pickle'.format(a=savingLoc), 'wb') as handle:\n",
    "    pickle.dump(trip_df, handle, protocol=pickle.HIGHEST_PROTOCOL)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>VEHICLE</th>\n",
       "      <th>activityType</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>start</th>\n",
       "      <th>end</th>\n",
       "      <th>duration</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>work</td>\n",
       "      <td>629393.6676188618</td>\n",
       "      <td>5803949.115843206</td>\n",
       "      <td>06:52:04</td>\n",
       "      <td>16:49:20</td>\n",
       "      <td>09:57:16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>home</td>\n",
       "      <td>632315.3322837545</td>\n",
       "      <td>5817000.086435355</td>\n",
       "      <td>17:10:47</td>\n",
       "      <td>1 days 06:29:59</td>\n",
       "      <td>13:19:12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10</td>\n",
       "      <td>work</td>\n",
       "      <td>627192.4415982522</td>\n",
       "      <td>5799465.4065392725</td>\n",
       "      <td>06:01:34</td>\n",
       "      <td>15:27:02</td>\n",
       "      <td>09:25:28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10</td>\n",
       "      <td>home</td>\n",
       "      <td>632315.3322837545</td>\n",
       "      <td>5817000.086435355</td>\n",
       "      <td>15:54:40</td>\n",
       "      <td>1 days 05:35:01</td>\n",
       "      <td>13:40:21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>100007</td>\n",
       "      <td>work</td>\n",
       "      <td>634456.1049276257</td>\n",
       "      <td>5793373.482339734</td>\n",
       "      <td>06:32:44</td>\n",
       "      <td>15:59:53</td>\n",
       "      <td>09:27:09</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  VEHICLE activityType                  x                   y     start  \\\n",
       "0       1         work  629393.6676188618   5803949.115843206  06:52:04   \n",
       "1       1         home  632315.3322837545   5817000.086435355  17:10:47   \n",
       "2      10         work  627192.4415982522  5799465.4065392725  06:01:34   \n",
       "3      10         home  632315.3322837545   5817000.086435355  15:54:40   \n",
       "4  100007         work  634456.1049276257   5793373.482339734  06:32:44   \n",
       "\n",
       "               end duration  \n",
       "0         16:49:20 09:57:16  \n",
       "1  1 days 06:29:59 13:19:12  \n",
       "2         15:27:02 09:25:28  \n",
       "3  1 days 05:35:01 13:40:21  \n",
       "4         15:59:53 09:27:09  "
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "activity_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we generate __training sets__ from trips and activities. This includes the trips and activity of 1% of the users. each training record is the label, duration and starting time of that event/activity. Then we use the entire data as the __test set__ to identify the location/activity category. The location category is either `stay`(activity) or `pass-by`(trip). The activity categories are either `home`, `work` or `other`. In this step we compute the identification accuracy which is the number of correctly identified calss devided by the number of each class. For instance we compute `home` identification accuracy as follows:\n",
    "$a = \\frac{true-positive-`home`}{actual-number-`home`s } $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('{a}activity_df.pickle'.format(a=savingLoc), 'rb') as handle:\n",
    "    activities = pickle.load(handle)\n",
    "with open(\"{a}trip_df.pickle\".format(a=savingLoc), 'rb') as handle:\n",
    "    trips = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\programdata\\anaconda3\\envs\\extractingodfromxml\\lib\\site-packages\\ipykernel_launcher.py:84: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "c:\\programdata\\anaconda3\\envs\\extractingodfromxml\\lib\\site-packages\\ipykernel_launcher.py:90: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "c:\\programdata\\anaconda3\\envs\\extractingodfromxml\\lib\\site-packages\\ipykernel_launcher.py:96: RuntimeWarning: divide by zero encountered in log\n",
      "c:\\programdata\\anaconda3\\envs\\extractingodfromxml\\lib\\site-packages\\ipykernel_launcher.py:96: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "c:\\programdata\\anaconda3\\envs\\extractingodfromxml\\lib\\site-packages\\ipykernel_launcher.py:98: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "c:\\programdata\\anaconda3\\envs\\extractingodfromxml\\lib\\site-packages\\ipykernel_launcher.py:124: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "c:\\programdata\\anaconda3\\envs\\extractingodfromxml\\lib\\site-packages\\ipykernel_launcher.py:130: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "c:\\programdata\\anaconda3\\envs\\extractingodfromxml\\lib\\site-packages\\ipykernel_launcher.py:136: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "c:\\programdata\\anaconda3\\envs\\extractingodfromxml\\lib\\site-packages\\ipykernel_launcher.py:138: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "c:\\programdata\\anaconda3\\envs\\extractingodfromxml\\lib\\site-packages\\ipykernel_launcher.py:146: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "c:\\programdata\\anaconda3\\envs\\extractingodfromxml\\lib\\site-packages\\ipykernel_launcher.py:152: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "c:\\programdata\\anaconda3\\envs\\extractingodfromxml\\lib\\site-packages\\ipykernel_launcher.py:158: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "c:\\programdata\\anaconda3\\envs\\extractingodfromxml\\lib\\site-packages\\ipykernel_launcher.py:160: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.0 minutes\n"
     ]
    }
   ],
   "source": [
    "#we time the process\n",
    "startTime = time.time()\n",
    "\n",
    "# _______________test seed based on user sampling _________________________________\n",
    "seedSet = np.arange(101,151)\n",
    "sensitivityTable = pd.DataFrame(index=seedSet)\n",
    "sensitivityTable.index=seedSet\n",
    "for s, seed in enumerate(seedSet):\n",
    "    ids = pd.unique(activities.VEHICLE)\n",
    "    random.seed(seed)\n",
    "    indices = random.sample(range(len(ids)),round(.01*len(ids))) #1% sampling of users\n",
    "    indices = pd.DataFrame(indices, columns=[\"VEHICLE\"])\n",
    "    trainIDs =pd.DataFrame(ids[indices])\n",
    "    trainIDs.columns = [\"VEHICLE\"]\n",
    "    trainingSet_trips = (trainIDs.merge(trips, on=[\"VEHICLE\"], how=\"inner\", sort=True,validate=\"1:m\"))\n",
    "    trainingSet_activities = (trainIDs.merge(activities, on=[\"VEHICLE\"], how=\"inner\", sort=True,validate=\"1:m\"))\n",
    "    with open('{a}/trainingActivity_seed{s}.pickle'.format(a=savingLoc,s=seed), 'wb') as handle:\n",
    "        pickle.dump(trainingSet_activities, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    with open('{a}trainingTrip_seed{s}.pickle'.format(a=savingLoc,s=seed), 'wb') as handle:\n",
    "        pickle.dump(trainingSet_trips, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "\n",
    "    trainingSet_home = trainingSet_activities[trainingSet_activities.activityType == \"home\"]\n",
    "    trainingSet_work = trainingSet_activities[trainingSet_activities.activityType == \"work\"]\n",
    "    trainingSet_other = trainingSet_activities[(trainingSet_activities.activityType != \"home\") & (trainingSet_activities.activityType != \"work\")]\n",
    "    trainingSet_home.to_csv(\"{a}trainingHome_seed{s}.CSV\".format(a=savingLoc,s=seed),index=False,header=True)\n",
    "    trainingSet_work.to_csv(\"{a}trainingWork_seed{s}.CSV\".format(a=savingLoc,s=seed),index=False,header=True)\n",
    "    trainingSet_other.to_csv(\"{a}trainingOther_seed{s}.CSV\".format(a=savingLoc,s=seed),index=False,header=True)\n",
    "\n",
    "    with open('{a}trainingHome_seed{s}.pickle'.format(a=savingLoc,s=seed), 'wb') as handle:\n",
    "        pickle.dump(trainingSet_home, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    with open('{a}trainingWork_seed{s}.pickle'.format(a=savingLoc,s=seed), 'wb') as handle:\n",
    "        pickle.dump(trainingSet_work, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    with open('{a}trainingOther_seed{s}.pickle'.format(a=savingLoc,s=seed), 'wb') as handle:\n",
    "        pickle.dump(trainingSet_other, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    # ________________________________________________________________________________________\n",
    "    # ****************************************************************************************\n",
    "    home = activities[activities.activityType== \"home\"]\n",
    "    work = activities[activities.activityType== \"work\"]\n",
    "    other = activities[(activities.activityType != \"home\") & (activities.activityType != \"work\")]\n",
    "    # ****************************************************************************************\n",
    "    # ________________________ writing data: Trainset _________________________\n",
    "\n",
    "    # ________________________________________________________________\n",
    "\n",
    "    # ___________________________ Probability calculations_________________________________________\n",
    "    #***************************************************************************\n",
    "    prior_activity = len(activities)/(len(activities) + len(trips))\n",
    "    prior_trip =  len(trips)/(len(activities) + len(trips))\n",
    "    activities['activity?'] = np.log(prior_trip+((gaussian_kde((\n",
    "        pd.to_timedelta(trainingSet_trips.start)).dt.total_seconds()).pdf((\n",
    "        pd.to_timedelta(activities.start)).dt.total_seconds())))+((gaussian_kde(\n",
    "        (pd.to_timedelta(trainingSet_trips.duration)).dt.total_seconds()).pdf((\n",
    "        pd.to_timedelta(activities.duration)).dt.total_seconds()))))<=np.log(prior_activity+((gaussian_kde((\n",
    "        pd.to_timedelta(trainingSet_activities.start)).dt.total_seconds()).pdf((\n",
    "        pd.to_timedelta(activities.start)).dt.total_seconds())))+((gaussian_kde(\n",
    "        (pd.to_timedelta(trainingSet_activities.duration)).dt.total_seconds()).pdf((\n",
    "        pd.to_timedelta(activities.duration)).dt.total_seconds()))))\n",
    "    sensitivityTable.loc[seed,\"stay accuracy\"] = sum(activities['activity?'])/len(activities)\n",
    "\n",
    "    #***************************************************************************\n",
    "    trips['trip?'] = np.log(prior_trip+((gaussian_kde((\n",
    "        pd.to_timedelta(trainingSet_trips.start)).dt.total_seconds()).pdf((\n",
    "        pd.to_timedelta(trips.start)).dt.total_seconds())))+((gaussian_kde(\n",
    "        (pd.to_timedelta(trainingSet_trips.duration)).dt.total_seconds()).pdf((\n",
    "        pd.to_timedelta(trips.duration)).dt.total_seconds()))))>=np.log(prior_activity+((gaussian_kde((\n",
    "        pd.to_timedelta(trainingSet_activities.start)).dt.total_seconds()).pdf((\n",
    "        pd.to_timedelta(trips.start)).dt.total_seconds())))+((gaussian_kde(\n",
    "        (pd.to_timedelta(trainingSet_activities.duration)).dt.total_seconds()).pdf((\n",
    "        pd.to_timedelta(trips.duration)).dt.total_seconds()))))\n",
    "    sensitivityTable.loc[seed,\"pass-by accuracy\"] = sum(trips['trip?'])/len(trips)\n",
    "\n",
    "\n",
    "    #***************************************************************************\n",
    "\n",
    "    prior_home = len(trainingSet_home)/(len(trainingSet_home) + len(trainingSet_work) + len(trainingSet_other))\n",
    "    prior_work =  len(trainingSet_work)/(len(trainingSet_home) + len(trainingSet_work) + len(trainingSet_other))\n",
    "    prior_other = len(trainingSet_other)/(len(trainingSet_home) + len(trainingSet_work) + len(trainingSet_other))\n",
    "\n",
    "    home['home'] = np.log(prior_home)+np.log(((gaussian_kde((\n",
    "        pd.to_timedelta(trainingSet_home.start)).dt.total_seconds()).pdf((\n",
    "        pd.to_timedelta(home.start)).dt.total_seconds()))))+np.log(((gaussian_kde(\n",
    "        (pd.to_timedelta(trainingSet_home.duration)).dt.total_seconds()).pdf((\n",
    "        pd.to_timedelta(home.duration)).dt.total_seconds()))))\n",
    "\n",
    "    home['work'] = np.log(prior_work)+np.log(((gaussian_kde((\n",
    "        pd.to_timedelta(trainingSet_work.start)).dt.total_seconds()).pdf((\n",
    "        pd.to_timedelta(home.start)).dt.total_seconds()))))+np.log(((gaussian_kde(\n",
    "        (pd.to_timedelta(trainingSet_work.duration)).dt.total_seconds()).pdf((\n",
    "        pd.to_timedelta(home.duration)).dt.total_seconds()))))\n",
    "\n",
    "    home['other'] = np.log(prior_other)+np.log(((gaussian_kde((\n",
    "        pd.to_timedelta(trainingSet_other.start)).dt.total_seconds()).pdf((\n",
    "        pd.to_timedelta(home.start)).dt.total_seconds()))))+np.log(((gaussian_kde(\n",
    "        (pd.to_timedelta(trainingSet_other.duration)).dt.total_seconds()).pdf((\n",
    "        pd.to_timedelta(home.duration)).dt.total_seconds()))))\n",
    "\n",
    "    home['activity?'] = home[['home','work','other']].idxmax(axis=1)\n",
    "#     (np.log(prior_home+((gaussian_kde((\n",
    "#         pd.to_timedelta(trainingSet_home.start)).dt.total_seconds()).pdf((\n",
    "#         pd.to_timedelta(home.start)).dt.total_seconds())))+((gaussian_kde(\n",
    "#         (pd.to_timedelta(trainingSet_trips.duration)).dt.total_seconds()).pdf((\n",
    "#         pd.to_timedelta(home.duration)).dt.total_seconds()))))>=np.log(prior_work+((gaussian_kde((\n",
    "#         pd.to_timedelta(trainingSet_work.start)).dt.total_seconds()).pdf((\n",
    "#         pd.to_timedelta(home.start)).dt.total_seconds())))+((gaussian_kde(\n",
    "#         (pd.to_timedelta(trainingSet_work.duration)).dt.total_seconds()).pdf((\n",
    "#         pd.to_timedelta(home.duration)).dt.total_seconds()))))) &(np.log(prior_home+((gaussian_kde((\n",
    "#         pd.to_timedelta(trainingSet_home.start)).dt.total_seconds()).pdf((\n",
    "#         pd.to_timedelta(home.start)).dt.total_seconds())))+((gaussian_kde(\n",
    "#         (pd.to_timedelta(trainingSet_home.duration)).dt.total_seconds()).pdf((\n",
    "#         pd.to_timedelta(home.duration)).dt.total_seconds()))))>=np.log(prior_other+((gaussian_kde((\n",
    "#         pd.to_timedelta(trainingSet_other.start)).dt.total_seconds()).pdf((\n",
    "#         pd.to_timedelta(home.start)).dt.total_seconds())))+((gaussian_kde(\n",
    "#         (pd.to_timedelta(trainingSet_other.duration)).dt.total_seconds()).pdf((\n",
    "#         pd.to_timedelta(home.duration)).dt.total_seconds())))))\n",
    "    sensitivityTable.loc[seed,\"home accuracy\"] = sum(home['activity?']==\"home\")/len(home)\n",
    "\n",
    "\n",
    "#***************************************************************************\n",
    "    work['home'] = np.log(prior_home)+np.log(((gaussian_kde((\n",
    "        pd.to_timedelta(trainingSet_home.start)).dt.total_seconds()).pdf((\n",
    "        pd.to_timedelta(work.start)).dt.total_seconds()))))+np.log(((gaussian_kde(\n",
    "        (pd.to_timedelta(trainingSet_home.duration)).dt.total_seconds()).pdf((\n",
    "        pd.to_timedelta(work.duration)).dt.total_seconds()))))\n",
    "\n",
    "    work['work'] = np.log(prior_work)+np.log(((gaussian_kde((\n",
    "        pd.to_timedelta(trainingSet_work.start)).dt.total_seconds()).pdf((\n",
    "        pd.to_timedelta(work.start)).dt.total_seconds()))))+np.log(((gaussian_kde(\n",
    "        (pd.to_timedelta(trainingSet_work.duration)).dt.total_seconds()).pdf((\n",
    "        pd.to_timedelta(work.duration)).dt.total_seconds()))))\n",
    "\n",
    "    work['other'] = np.log(prior_other)+np.log(((gaussian_kde((\n",
    "        pd.to_timedelta(trainingSet_other.start)).dt.total_seconds()).pdf((\n",
    "        pd.to_timedelta(work.start)).dt.total_seconds()))))+np.log(((gaussian_kde(\n",
    "        (pd.to_timedelta(trainingSet_other.duration)).dt.total_seconds()).pdf((\n",
    "        pd.to_timedelta(work.duration)).dt.total_seconds()))))\n",
    "\n",
    "    work['activity?'] = work[['home','work','other']].idxmax(axis=1)\n",
    "    sensitivityTable.loc[seed,\"work accuracy\"] = sum(work['activity?']==\"work\")/len(work)\n",
    "\n",
    "#***************************************************************************\n",
    "    other['home'] = np.log(prior_home)+np.log(((gaussian_kde((\n",
    "        pd.to_timedelta(trainingSet_home.start)).dt.total_seconds()).pdf((\n",
    "        pd.to_timedelta(other.start)).dt.total_seconds()))))+np.log(((gaussian_kde(\n",
    "        (pd.to_timedelta(trainingSet_home.duration)).dt.total_seconds()).pdf((\n",
    "        pd.to_timedelta(other.duration)).dt.total_seconds()))))\n",
    "\n",
    "    other['work'] = np.log(prior_work)+np.log(((gaussian_kde((\n",
    "        pd.to_timedelta(trainingSet_work.start)).dt.total_seconds()).pdf((\n",
    "        pd.to_timedelta(other.start)).dt.total_seconds()))))+np.log(((gaussian_kde(\n",
    "        (pd.to_timedelta(trainingSet_work.duration)).dt.total_seconds()).pdf((\n",
    "        pd.to_timedelta(other.duration)).dt.total_seconds()))))\n",
    "\n",
    "    other['other'] = np.log(prior_other)+np.log(((gaussian_kde((\n",
    "        pd.to_timedelta(trainingSet_other.start)).dt.total_seconds()).pdf((\n",
    "        pd.to_timedelta(other.start)).dt.total_seconds()))))+np.log(((gaussian_kde(\n",
    "        (pd.to_timedelta(trainingSet_other.duration)).dt.total_seconds()).pdf((\n",
    "        pd.to_timedelta(other.duration)).dt.total_seconds()))))\n",
    "\n",
    "    other['activity?'] = other[['home','work','other']].idxmax(axis=1)\n",
    "    sensitivityTable.loc[seed,\"other accuracy\"] = sum(other['activity?']==\"other\")/len(other)\n",
    "\n",
    "#     ***************************************************************************\n",
    "    home.to_csv(\"{a}home_seed{s}.CSV\".format(a=savingLoc,s=seed),index=False,header=True)\n",
    "    work.to_csv(\"{a}work_seed{s}.CSV\".format(a=savingLoc,s=seed),index=False,header=True)\n",
    "    other.to_csv(\"{a}other_seed{s}.CSV\".format(a=savingLoc,s=seed),index=False,header=True)\n",
    "\n",
    "    with open('{a}home_seed{s}.pickle'.format(a=savingLoc,s=seed), 'wb') as handle:\n",
    "        pickle.dump(home, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    with open('{a}work_seed{s}.pickle'.format(a=savingLoc,s=seed), 'wb') as handle:\n",
    "        pickle.dump(work, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    with open('{a}other_seed{s}.pickle'.format(a=savingLoc,s=seed), 'wb') as handle:\n",
    "        pickle.dump(other, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "sensitivityTable.to_excel(\"{a}userSampling_onePercentLocationDetectionSensitivity.xlsx\".format(a=savingLoc), header=True,\n",
    "                          index=True)\n",
    "print((time.time() - startTime)//60,'minutes')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
